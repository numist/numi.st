---
layout: post
title: Caches as Re-Reference Interval Predictors
excerpt: "How a paper about CPU caches changed the way I think about software caches too"
---

It's funny how hardware and software are able to solve the same problems in dramatically different ways. Want to add a 2-bit counter to every slot in your cache, then find the first slot with a counter value of `3`—incrementing them all at once until a suitable slot is found—in constant time? Sure, why not! It's just transistors! They excel at doing a bunch of stuff in parallel, it just costs die space and power![^141]

The "2-bit counter per cache slot" thing isn't a random example, Intel _actually does this_ in their <abbr title="Last-Level Cache">LLC</abbr>. They even [published a paper about it](Jaleel et al, 2010) and the benchmark results are pretty solid: a hit rate 6-10% better than <abbr title="Least Recently Used">LRU</abbr>. But the paper also offers a deeper understanding of caching.

## What's a Cache?

Backing up for a moment, a cache is a space to store things you've used in case you need them again, but it's always too small and can't tell the future so it uses eviction policies like "least recently used" or "least frequently used" to decide what stays in the cache and what doesn't in the hope of maximizing cache performance, as measured by its "hit rate". Each of these eviction policies has some bias that tends to be obvious from its name.

An LRU, with its bias toward recency of use, can be implemented as a list of slots. On a "miss" a slot is evicted from the front of the list to make room for the new value's slot, which is appended to the end. On a cache "hit" the existing value's slot is moved to the end of the list. This way the overall contents of the list always gradually move toward the front, pushed in that direction by hot values being moved (or inserted) at the end.

```
TODO: some graphviz stuff here that people can interact with
```

## Making Better Predictions

The <abbr title="Re-Reference Interval Prediction">RRIP</abbr> paper posits: what if caches made reuse predictions more nuanced than "immediately" and "probably never"? The way it accomplishes intermediate predictions can be conceptualized as inserting new values somewhere in the middle[^middle] of the list.

Based on this concept, a new eviction policy predicts values are not likely to be reused again _unless they have been reused in the past_, accomplishing this by inserting new values near the front of the list and promoting them towards the end when they're hit. Another variation adds randomization to new entries' insertion position.

## In Software

Of course caches are commonly found in software, too. Some things are costly to compute on demand, but the system's memory is too finite to store the results of every computation forever. As a twist, software systems often have an advantage over general purpose hardware when it comes to caches: sometimes they can use domain-specific knowledge to make more informed re-reference interval predictions. A great example of this is binary trees—every operation uses the root node, but a random leaf's probability of participating in a search is ¹⁄ₙ!

Associating a counter with each slot leads to some pretty heinous software, but the concept can be expressed just as effectively with a ring buffer:

``` swift

```



[^141]: I'm being glib here, _of course_ there's a limit. In college two friends and I designed an application-specific CPU with an instruction so complex the theoretical maximum clock speed would have been just north of 4MHz, but clock speed wasn't part of the grade—only number of cycles to solve the problem!
[^middle]: More concretely, _m_-bit counters give you _2<sup>m</sup>_ distinct insertion points into the cache<!--. They may not be evenly spaced, but all entries are guaranteed to make progress towards eviction as long as you don't insert into the 2<sup>m</sup>-1º (where the evictions happen)-->